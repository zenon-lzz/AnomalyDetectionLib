{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MtsCID Tutorial for Anomaly Detection\n",
    "This notebook demonstrates how to use MtsCID for time series anomaly detection\n",
    "\n",
    "## 1. Packages import and prepare arguments"
   ],
   "id": "ddfd2aef939a20b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tsadlib.configs.constants import PROJECT_ROOT\n",
    "from tsadlib.configs.type import ConfigType\n",
    "from tsadlib.data_provider.data_factory import data_provider\n",
    "from tsadlib.metrics.threshold import percentile_threshold\n",
    "from tsadlib.models.MtsCID import MtsCID\n",
    "from tsadlib.utils.adjustment import point_adjustment\n",
    "from tsadlib.utils.logger import logger\n",
    "from tsadlib.utils.loss import EntropyLoss, GatheringLoss, harmonic_loss_compute\n",
    "from tsadlib.utils.lr_decay import PolynomialDecayLR\n",
    "from tsadlib.utils.traning_stoper import OneEarlyStopping\n",
    "\n",
    "# Set up device for computation (CUDA GPU, Apple M1/M2 GPU, or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "logger.info(f'use device: {device}')\n",
    "device = torch.device(device)\n",
    "\n",
    "# Define paths for dataset and model checkpoints\n",
    "# DATASET_ROOT = 'E:\\\\liuzhenzhou\\\\datasets'\n",
    "# DATASET_ROOT = '/Users/liuzhenzhou/Documents/backup/datasets/anomaly_detection/npy'\n",
    "DATASET_ROOT = '/home/lzz/Desktop/datasets'\n",
    "DATASET_TYPE = 'MSL'  # Mars Science Laboratory dataset\n",
    "MODEL = 'MtsCID'\n",
    "CHECKPOINTS = os.path.join(PROJECT_ROOT, 'checkpoints', MODEL)\n",
    "\n",
    "writer = SummaryWriter(os.path.join(PROJECT_ROOT, 'runs', MODEL).__str__())\n",
    "\n",
    "# Configure TimesNet hyperparameters and training settings\n",
    "args = ConfigType(**{\n",
    "    'model': MODEL,\n",
    "    'mode': 'train',\n",
    "    'dataset_root_path': os.path.join(DATASET_ROOT, DATASET_TYPE),\n",
    "    'window_size': 100,\n",
    "    'batch_size': 64,\n",
    "    'd_model': 55,\n",
    "    'encoder_layers': 1,\n",
    "    'input_channels': 55,\n",
    "    'output_channels': 55,\n",
    "    'dropout': 0.1,\n",
    "    'anomaly_ratio': 1\n",
    "})\n",
    "\n",
    "# Load training and testing data\n",
    "train_loader, validate_loader, test_loader = data_provider(args, split_way='train_validate_split')"
   ],
   "id": "b1a76d27b31d602a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Model definition and training\n",
    "\n",
    "### 2.1 First Training for optimize encoder"
   ],
   "id": "798db0b33c1d7572"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize model and training components\n",
    "model = MtsCID(args).to(device)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.peak_lr,\n",
    "                              weight_decay=args.weight_decay)\n",
    "scheduler = PolynomialDecayLR(optimizer, warmup_steps=args.warmup_epoch * args.batch_size,\n",
    "                              total_steps=args.num_epochs * args.batch_size, lr=args.learning_rate,\n",
    "                              end_lr=args.end_learning_rate, power=1.0)\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "entropy_criterion = EntropyLoss()\n",
    "early_stopping = OneEarlyStopping(args.patience, CHECKPOINTS, DATASET_TYPE)\n",
    "train_steps = len(train_loader)\n",
    "logger.info('The training phase starts')\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    reconstruct_losses = []\n",
    "    entropy_losses = []\n",
    "    validate_losses = []\n",
    "    iter_count = 0\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for i, (x_data, _) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch + 1} / {args.num_epochs}')):\n",
    "        iter_count += 1\n",
    "        optimizer.zero_grad()\n",
    "        x_data = x_data.float().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output_dict = model(x_data)\n",
    "        output, attention = output_dict['output'], output_dict['attention']\n",
    "        reconstruct_loss = criterion(output, x_data)\n",
    "        entropy_loss = entropy_criterion(attention)\n",
    "        loss = reconstruct_loss + entropy_loss\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            writer.add_scalar('Loss/Train', loss.item(), epoch * train_steps + i)\n",
    "            writer.add_scalar('Loss/Reconstruct', reconstruct_loss.item(), epoch * train_steps + i)\n",
    "            writer.add_scalar('Loss/Entropy', entropy_loss.item(), epoch * train_steps + i)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        reconstruct_losses.append(reconstruct_loss.item())\n",
    "        entropy_losses.append(entropy_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x_data, _) in enumerate(validate_loader):\n",
    "            x_data = x_data.float().to(device)\n",
    "            # Forward pass\n",
    "            output_dict = model(x_data)\n",
    "            output, attention = output_dict['output'], output_dict['attention']\n",
    "            reconstruct_loss = criterion(output, x_data)\n",
    "            entropy_loss = entropy_criterion(attention)\n",
    "            loss = reconstruct_loss + args.hyper_parameter_lambda * entropy_loss\n",
    "            validate_losses.append(loss.item())\n",
    "\n",
    "    train_avg_loss = np.average(train_losses)\n",
    "    validate_avg_loss = np.average(validate_losses)\n",
    "    logger.info(\"Epoch: {:>2} cost time: {:<10.4f}s, train loss: {:<.7f}, validate loss: {:<.7f}\", epoch + 1,\n",
    "                time.time() - epoch_time, train_avg_loss, validate_avg_loss)\n",
    "\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": train_avg_loss, \"Validation\": validate_avg_loss}, epoch)\n",
    "\n",
    "    # Early stopping check\n",
    "    early_stopping(validate_avg_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        logger.warning(\"Early stopping triggered\")\n",
    "        break"
   ],
   "id": "7fcc9b7cc6440c1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Model Evaluation",
   "id": "db938d0c88248d42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_energy = []\n",
    "validate_energy = []\n",
    "test_energy = []\n",
    "test_labels = []\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "gathering_loss = GatheringLoss(reduce=False)\n",
    "temperature = args.temperature\n",
    "\n",
    "logger.info('Test Phase Starts')\n",
    "\n",
    "model.eval()\n",
    "# Calculate Anomaly Scores in training set.\n",
    "for i, (x_data, _) in enumerate(train_loader):\n",
    "    x_data = x_data.float().to(device)\n",
    "\n",
    "    output_dict = model(x_data)\n",
    "    output, queries, memory = output_dict['output'], output_dict['queries'], output_dict['memory']\n",
    "\n",
    "    # calculate loss and anomaly scores\n",
    "    reconstruct_loss = criterion(x_data, output)\n",
    "    latent_score = torch.softmax(gathering_loss(queries, memory) / temperature, dim=-1)\n",
    "    loss = harmonic_loss_compute(reconstruct_losses, latent_score, 'normal_mean')\n",
    "\n",
    "    train_energy.append(loss.detach().cpu().numpy())\n",
    "\n",
    "train_energy = np.concatenate(train_energy, axis=0).reshape(-1)\n",
    "# Calculate Anomaly Scores in validation set.\n",
    "for i, (x_data, _) in enumerate(validate_loader):\n",
    "    x_data = x_data.float().to(device)\n",
    "\n",
    "    output_dict = model(x_data)\n",
    "    output, queries, memory = output_dict['output'], output_dict['queries'], output_dict['memory']\n",
    "\n",
    "    # calculate loss and anomaly scores\n",
    "    reconstruct_loss = criterion(x_data, output)\n",
    "    latent_score = torch.softmax(gathering_loss(queries, memory) / temperature, dim=-1)\n",
    "    loss = harmonic_loss_compute(reconstruct_losses, latent_score, 'normal_mean')\n",
    "\n",
    "    validate_energy.append(loss.detach().cpu().numpy())\n",
    "\n",
    "validate_energy = np.concatenate(validate_energy, axis=0).reshape(-1)\n",
    "combined_energy = np.concatenate([train_energy, validate_energy], axis=0)\n",
    "threshold = percentile_threshold(combined_energy, 100 - args.anomaly_ratio)\n",
    "logger.info('Threshold is {:.4f}', threshold)\n",
    "\n",
    "# Calculate reconstruction scores for test data\n",
    "for i, (x_data, labels) in enumerate(test_loader):\n",
    "    x_data = x_data.float().to(device)\n",
    "    output_dict = model(x_data)\n",
    "    output, queries, memory = output_dict['output'], output_dict['queries'], output_dict['memory']\n",
    "\n",
    "    reconstruct_loss = criterion(x_data, output)\n",
    "    latent_score = torch.softmax(gathering_loss(queries, memory) / temperature, dim=-1)\n",
    "    loss = harmonic_loss_compute(reconstruct_losses, latent_score, 'normal_mean')\n",
    "    test_energy.append(loss.detach().cpu().numpy())\n",
    "    test_labels.append(labels)\n",
    "\n",
    "# Combine scores and labels from all batches\n",
    "test_energy = np.concatenate(test_energy, axis=0).reshape(-1)  # [total_samples, window_size]\n",
    "test_labels = np.concatenate(test_labels, axis=0).reshape(-1)  # [total_samples, window_size]\n",
    "\n",
    "# Generate predictions based on threshold\n",
    "pred_labels = (test_energy > threshold).astype(int)\n",
    "gt_labels = test_labels.astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(gt_labels, pred_labels, average='binary')\n",
    "logger.success('Before point-adjustment:\\nPrecision: {:.2f}\\nRecall: {:.4f}\\nF1-score: {:.2f}', precision, recall, f1)\n",
    "\n",
    "# Apply point-adjustment strategy\n",
    "gt, pred = point_adjustment(test_labels, pred_labels)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(gt, pred, average='binary')\n",
    "logger.success('After point-adjustment:\\nPrecision: {:.2f}\\nRecall: {:.4f}\\nF1-score: {:.2f}', precision, recall, f1)"
   ],
   "id": "d7306ca7754f4fd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Results Visualization",
   "id": "6d436005aa297dd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(test_energy)\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label='threshold')\n",
    "anomaly_indices = np.where(gt == 1)[0]\n",
    "plt.plot(np.arange(len(test_energy))[anomaly_indices], test_energy[anomaly_indices], 'r.', markersize=2,\n",
    "         label='Anomaly')\n",
    "plt.title('MEMTO Model Evaluation')\n",
    "plt.xlabel('TimeStamp')\n",
    "plt.ylabel('Anomaly Scores')\n",
    "plt.legend()"
   ],
   "id": "7ddfd519addf24ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
