{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MEMTO Tutorial for Anomaly Detection\n",
    "This notebook demonstrates how to use MEMTO for time series anomaly detection\n",
    "\n",
    "## 1. Packages import and prepare arguments"
   ],
   "id": "e9f29d797a0a5d69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tsadlib.configs.constants import PROJECT_ROOT\n",
    "from tsadlib.configs.type import ConfigType\n",
    "from tsadlib.data_provider.data_factory import data_provider\n",
    "from tsadlib.metrics.threshold import percentile_threshold\n",
    "from tsadlib.models.MEMTO import MEMTO\n",
    "from tsadlib.utils.adjustment import point_adjustment\n",
    "from tsadlib.utils.clustering import k_means_clustering\n",
    "from tsadlib.utils.logger import logger\n",
    "from tsadlib.utils.loss import EntropyLoss, GatheringLoss\n",
    "from tsadlib.utils.traning_stoper import OneEarlyStopping\n",
    "\n",
    "# Set up device for computation (CUDA GPU, Apple M1/M2 GPU, or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'use device: {device}')\n",
    "device = torch.device(device)\n",
    "\n",
    "# Define paths for dataset and model checkpoints\n",
    "# DATASET_ROOT = 'E:\\\\liuzhenzhou\\\\datasets'\n",
    "# DATASET_ROOT = '/Users/liuzhenzhou/Documents/backup/datasets/anomaly_detection/npy'\n",
    "DATASET_ROOT = '/home/lzz/Desktop/datasets'\n",
    "DATASET_TYPE = 'MSL'  # Mars Science Laboratory dataset\n",
    "MODEL = 'MEMTO'\n",
    "CHECKPOINTS = os.path.join(PROJECT_ROOT, 'checkpoints', MODEL)\n",
    "\n",
    "writer = SummaryWriter(os.path.join(PROJECT_ROOT, 'runs', MODEL).__str__())\n",
    "\n",
    "# Configure TimesNet hyperparameters and training settings\n",
    "args = ConfigType(**{\n",
    "    'model': MODEL,\n",
    "    'mode': 'train',\n",
    "    'dataset_root_path': os.path.join(DATASET_ROOT, DATASET_TYPE),\n",
    "    'window_size': 100,\n",
    "    'batch_size': 256,\n",
    "    'd_model': 8,\n",
    "    'dimension_fcl': 16,\n",
    "    'encoder_layers': 3,\n",
    "    'input_channels': 55,\n",
    "    'output_channels': 55,\n",
    "    'num_memory': 10,\n",
    "    'hyper_parameter_lambda': 0.01,\n",
    "    'dropout': 0.1,\n",
    "    'anomaly_ratio': 1,\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 1e-4\n",
    "})\n",
    "\n",
    "# Load training and testing data\n",
    "train_loader, validate_loader, test_loader, k_loader = data_provider(args, split_way='train_validate_k_split')"
   ],
   "id": "a1c4e9176d8fa979"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Model definition and training\n",
    "\n",
    "### 2.1 First Training for optimize encoder"
   ],
   "id": "6ae9b6e6b548bba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T03:43:49.582463Z",
     "start_time": "2025-03-29T03:43:49.507015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize model and training components\n",
    "model = MEMTO(args).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "entropy_loss = EntropyLoss()\n",
    "early_stopping = OneEarlyStopping(args.patience, CHECKPOINTS, DATASET_TYPE)\n",
    "train_steps = len(train_loader)\n",
    "logger.info('The first phase training starts')\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    reconstruct_losses = []\n",
    "    entropy_losses = []\n",
    "    validate_losses = []\n",
    "    iter_count = 0\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for i, (x_data, _) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch + 1} / {args.num_epochs}')):\n",
    "        iter_count += 1\n",
    "        optimizer.zero_grad()\n",
    "        x_data = x_data.float().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output_dict = model(x_data)\n",
    "        output, attention = output_dict['output'], output_dict['attention']\n",
    "        reconstruct_loss = criterion(output, x_data)\n",
    "        entropy_loss = entropy_loss(attention)\n",
    "        loss = reconstruct_loss + args.hyper_parameter_lambda * entropy_loss\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            writer.add_scalar('Loss/Train', loss.item(), epoch * train_steps + i)\n",
    "            writer.add_scalar('Loss/Reconstruct', reconstruct_loss.item(), epoch * train_steps + i)\n",
    "            writer.add_scalar('Loss/Entropy', entropy_loss.item(), epoch * train_steps + i)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        reconstruct_losses.append(reconstruct_loss.item())\n",
    "        entropy_losses.append(entropy_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x_data, _) in enumerate(validate_loader):\n",
    "            x_data = x_data.float().to(device)\n",
    "            # Forward pass\n",
    "            output_dict = model(x_data)\n",
    "            output, attention = output_dict['output'], output_dict['attention']\n",
    "            reconstruct_loss = criterion(output, x_data)\n",
    "            entropy_loss = entropy_loss(attention)\n",
    "            loss = reconstruct_loss + args.hyper_parameter_lambda * entropy_loss\n",
    "            validate_losses.append(loss.item())\n",
    "\n",
    "    train_avg_loss = np.average(train_losses)\n",
    "    validate_avg_loss = np.average(validate_losses)\n",
    "    logger.info(\"Epoch: {:>2} cost time: {:<10.4f}s, train loss: {:<.7f}, validate loss: {:<.7f}\", epoch + 1,\n",
    "                time.time() - epoch_time, train_avg_loss, validate_avg_loss)\n",
    "\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": train_avg_loss, \"Validation\": validate_avg_loss}, epoch)\n",
    "\n",
    "    # Early stopping check\n",
    "    if early_stopping(validate_avg_loss, model):\n",
    "        logger.info(\"Early stopping triggered\")\n",
    "        break\n"
   ],
   "id": "dfbea324ca117527",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MEMTO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Initialize model and training components\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m model = \u001B[43mMEMTO\u001B[49m(args).to(device)\n\u001B[32m      3\u001B[39m optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n\u001B[32m      4\u001B[39m criterion = torch.nn.MSELoss()\n",
      "\u001B[31mNameError\u001B[39m: name 'MEMTO' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Second training for reducing a risk of instability",
   "id": "fec7bf384c0f7d06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "outputs = []\n",
    "\n",
    "# sample 10% of training data to generate queries\n",
    "with torch.no_grad():\n",
    "    for i, (x_data, _) in enumerate(k_loader):\n",
    "        x_data = x_data.float().to(device)\n",
    "        # Forward pass\n",
    "        outputs.append(model(x_data)['queries'])\n",
    "\n",
    "# Apply K-means clustering algorithm to cluster the queries and designate each centroid as initial value of a memory item.\n",
    "outputs = torch.cat(outputs, dim=0)\n",
    "memory_init_embedding = k_means_clustering(outputs, args.num_memory, args.d_model)\n",
    "\n",
    "model = MEMTO(args, memory_init_embedding.detach()).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "early_stopping = OneEarlyStopping(args.patience, CHECKPOINTS, DATASET_TYPE)\n",
    "\n",
    "logger.info('The second phase training starts')\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    reconstruct_losses = []\n",
    "    entropy_losses = []\n",
    "    validate_losses = []\n",
    "    iter_count = 0\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for i, (x_data, _) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch + 1} / {args.num_epochs}')):\n",
    "        iter_count += 1\n",
    "        optimizer.zero_grad()\n",
    "        x_data = x_data.float().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output_dict = model(x_data)\n",
    "        output, attention = output_dict['output'], output_dict['attention']\n",
    "        reconstruct_loss = criterion(output, x_data)\n",
    "        entropy_loss = entropy_loss(attention)\n",
    "        loss = reconstruct_loss + args.hyper_parameter_lambda * entropy_loss\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            writer.add_scalar('Loss/Train', loss.item(), epoch * train_steps + i)\n",
    "            writer.add_scalar('Loss/Reconstruct', reconstruct_loss.item(), epoch * train_steps + i)\n",
    "            writer.add_scalar('Loss/Entropy', entropy_loss.item(), epoch * train_steps + i)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        reconstruct_losses.append(reconstruct_loss.item())\n",
    "        entropy_losses.append(entropy_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x_data, _) in enumerate(validate_loader):\n",
    "            x_data = x_data.float().to(device)\n",
    "            # Forward pass\n",
    "            output_dict = model(x_data)\n",
    "            output, attention = output_dict['output'], output_dict['attention']\n",
    "            reconstruct_loss = criterion(output, x_data)\n",
    "            entropy_loss = entropy_loss(attention)\n",
    "            loss = reconstruct_loss + args.hyper_parameter_lambda * entropy_loss\n",
    "            validate_losses.append(loss.item())\n",
    "\n",
    "    train_avg_loss = np.average(train_losses)\n",
    "    validate_avg_loss = np.average(validate_losses)\n",
    "    logger.info(\"Epoch: {:>2} cost time: {:<10.4f}s, train loss: {:<.7f}, validate loss: {:<.7f}\", epoch + 1,\n",
    "                time.time() - epoch_time, train_avg_loss, validate_avg_loss)\n",
    "\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": train_avg_loss, \"Validation\": validate_avg_loss}, epoch)\n",
    "\n",
    "    # Early stopping check\n",
    "    if early_stopping(validate_avg_loss, model):\n",
    "        logger.info(\"Early stopping triggered\")\n",
    "        break\n"
   ],
   "id": "bdab567fae3e6d74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Model Evaluation",
   "id": "1614a7e2577947db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_attention_energy = []\n",
    "validate_attention_energy = []\n",
    "test_attention_energy = []\n",
    "test_labels = []\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "gathering_loss = GatheringLoss(reduce=False)\n",
    "temperature = args.temperature\n",
    "\n",
    "logger.info('Test Phase Starts')\n",
    "\n",
    "model.eval()\n",
    "# Calculate Anomaly Scores in training set.\n",
    "for i, (x_data, _) in enumerate(train_loader):\n",
    "    x_data = x_data.float().to(device)\n",
    "\n",
    "    output_dict = model(x_data)\n",
    "    output, queries, memory = output_dict['output'], output_dict['queries'], output_dict['memory']\n",
    "\n",
    "    # calculate loss and anomaly scores\n",
    "    reconstruct_loss = torch.mean(criterion(x_data, output), dim=-1)\n",
    "    latent_score = torch.softmax(gathering_loss(queries, memory) / temperature, dim=-1)\n",
    "    loss = latent_score * reconstruct_loss\n",
    "\n",
    "    train_attention_energy.append(loss.detach().cpu().numpy())\n",
    "\n",
    "train_attention_energy = np.concatenate(train_attention_energy, axis=0).reshape(-1)\n",
    "# Calculate Anomaly Scores in validation set.\n",
    "for i, (x_data, _) in enumerate(validate_loader):\n",
    "    x_data = x_data.float().to(device)\n",
    "\n",
    "    output_dict = model(x_data)\n",
    "    output, queries, memory = output_dict['output'], output_dict['queries'], output_dict['memory']\n",
    "\n",
    "    # calculate loss and anomaly scores\n",
    "    reconstruct_loss = torch.mean(criterion(x_data, output), dim=-1)\n",
    "    latent_score = torch.softmax(gathering_loss(queries, memory) / temperature, dim=-1)\n",
    "    loss = latent_score * reconstruct_loss\n",
    "\n",
    "    validate_attention_energy.append(loss.detach().cpu().numpy())\n",
    "\n",
    "validate_attention_energy = np.concatenate(validate_attention_energy, axis=0).reshape(-1)\n",
    "combined_energy = np.concatenate([train_attention_energy, validate_attention_energy], axis=0)\n",
    "threshold = percentile_threshold(combined_energy, 100 - args.anomaly_ratio)\n",
    "logger.info('Threshold is {.4f}', threshold)\n",
    "\n",
    "# Calculate reconstruction scores for test data\n",
    "for i, (x_data, labels) in enumerate(test_loader):\n",
    "    x_data = x_data.float().to(device)\n",
    "    output_dict = model(input)\n",
    "    output, queries, memory = output_dict['output'], output_dict['queries'], output_dict['memory']\n",
    "\n",
    "    reconstruct_loss = torch.mean(criterion(x_data, output), dim=-1)\n",
    "    latent_score = torch.softmax(gathering_loss(queries, memory) / temperature, dim=-1)\n",
    "    loss = latent_score * reconstruct_loss\n",
    "    test_attention_energy.append(loss.detach().cpu().numpy())\n",
    "    test_labels.append(labels)\n",
    "\n",
    "# Combine scores and labels from all batches\n",
    "test_energy = np.concatenate(test_attention_energy, axis=0).reshape(-1)  # [total_samples, window_size]\n",
    "test_labels = np.concatenate(test_labels, axis=0).reshape(-1)  # [total_samples, window_size]\n",
    "\n",
    "# Generate predictions based on threshold\n",
    "pred_labels = (test_energy > threshold).astype(int)\n",
    "gt_labels = test_labels.astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(gt_labels, pred_labels, average='binary')\n",
    "logger.success('Before point-adjustment:\\nPrecision: {:.2f}\\nRecall: {:.4f}\\nF1-score: {:.2f}', precision, recall, f1)\n",
    "\n",
    "# Apply point-adjustment strategy\n",
    "gt, pred = point_adjustment(test_labels, pred_labels)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(gt, pred, average='binary')\n",
    "logger.success('After point-adjustment:\\nPrecision: {:.2f}\\nRecall: {:.4f}\\nF1-score: {:.2f}', precision, recall, f1)"
   ],
   "id": "91a563df8446b78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Results Visualization",
   "id": "40d1d6ef00d31482"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(test_energy)\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label='threshold')\n",
    "anomaly_indices = np.where(gt == 1)[0]\n",
    "plt.plot(np.arange(len(test_energy))[anomaly_indices], test_energy[anomaly_indices], 'r.', markersize=2,\n",
    "         label='Anomaly')\n",
    "plt.title('MEMTO Model Evaluation')\n",
    "plt.xlabel('TimeStamp')\n",
    "plt.ylabel('Anomaly Scores')\n",
    "plt.legend()"
   ],
   "id": "2a7167fed3c66155"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
