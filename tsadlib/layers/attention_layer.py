"""
=================================================
@Author: Zenon
@Date: 2025-03-26
@Description: Attention Mechanism Layer Implementation
    This module implements various attention mechanism calculation methods, including:
    1. Self-Attention
    2. Multi-Head Attention
    
    These attention mechanisms are encapsulated as PyTorch layers that can be
    directly called by other model components. They are primarily used to capture
    dependencies across different dimensions in time series anomaly detection models.
==================================================
"""

